{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LanguageModel_gpt2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNwyK4tCQcwBSQG2A/a9kiD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Y5_R-NvzW-Na"},"source":["# First: Download a pre-trained model"]},{"cell_type":"code","metadata":{"id":"E4Xa16ScS6Md"},"source":["!git clone https://github.com/graykode/gpt-2-Pytorch\n","%cd gpt-2-Pytorch\n","!curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eGTDjahQXGUI"},"source":["# Import the required libraries to load and execute the model"]},{"cell_type":"code","metadata":{"id":"qONuVkVOSKmW"},"source":["import torch\n","import random\n","import numpy as np\n","from GPT2.model import (GPT2LMHeadModel)\n","from GPT2.utils import load_weight\n","from GPT2.config import GPT2Config\n","from GPT2.sample import sample_sequence\n","from GPT2.encoder import get_encoder\n","\n","state_dict = torch.load('gpt2-pytorch_model.bin', map_location='cpu' if not torch.cuda.is_available() else None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xYjvFo5UXPG4"},"source":["# Define a pipeline to process the input and run the model with it"]},{"cell_type":"code","metadata":{"id":"c3WD7e3UPRt8"},"source":["# text: is the text input that will go in the model\n","# length: how many words we want to predict\n","# top-k: returns the k highest probabilities, this represents how specific you want the predictions to be\n","def text_generator(text, length = -1, top_k = 40):\n","\n","  # these can be modified. For advanced testing\n","  nsamples = 1\n","  temperature = 0.7\n","  batch_size = 1\n","\n","  assert nsamples % batch_size == 0\n","\n","  seed = random.randint(0, 2147483647)\n","  np.random.seed(seed)\n","  torch.random.manual_seed(seed)\n","  torch.cuda.manual_seed(seed)\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","  # Load Model\n","  enc = get_encoder()\n","  config = GPT2Config()\n","  model = GPT2LMHeadModel(config)\n","  model = load_weight(model, state_dict)\n","  model.to(device)\n","  model.eval()\n","\n","  if length == -1:\n","      length = config.n_ctx // 2\n","  elif length > config.n_ctx:\n","      raise ValueError(\"Can't get samples longer than window size: %s\" % config.n_ctx)\n","\n","  context_tokens = enc.encode(text)\n","\n","  generated = 0\n","  for _ in range(nsamples // batch_size):\n","    out = sample_sequence(\n","      model = model, length=length,\n","      context = context_tokens,\n","      start_token = None,\n","      batch_size = batch_size,\n","      temperature = temperature, top_k=top_k, device=device\n","    )\n","    out = out[:, len(context_tokens):].tolist()\n","    print(\"\\n\"+text)\n","    for i in range(batch_size):\n","      generated += 1\n","      text = enc.decode(out[i]).replace(\"<|endoftext|>\", \". \")\n","      print(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d4nLLxKSPRwm"},"source":["text_generator(\"I usually walk my dog in the morning\", length=30, top_k=25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQBlEFZ3PRzK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UgL8U1pPKSG1"},"source":[""],"execution_count":null,"outputs":[]}]}